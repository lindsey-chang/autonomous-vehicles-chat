**7 规划和控制**

**7.1 简介**

如图7.1所示，为了生成车辆实时运动的信息，规划控制模块结合了感知模块输入的动态障碍物的实时信息、定位模块中车辆实时位置信息、以及地图模块中道路状态和静态障碍物的信息。基于以上模块的输入信息，规划控制模块可以生成车辆的运动规划策略。

![图片](Aspose.Words.f368d683-7bd0-4d9b-9cf6-d1b104590435.001.png)

图 7.1 模块化设计架构

正如在[1,2]中所详述的那样,一个典型的规划控制模块具有以下结构(图 7.2)。首先，当用户输入目的地（Destination）时，路径规划（Route Planning）模块可以通过校验地图上的道路网络（Road network）生成最佳路径。接着，将生成的路径传输至行为决策模块（Behavioral Planning），此模块会校验交通规则（Traffic rules）并生成运动规范（Motion specification）。随后将生成的路径点集序列（Sequence of waypoints）和运动规范传输至运动规划（Motion Planning）模块，由此模块结合感知障碍物信息（perceived obstacles）和实时车辆姿势(Real-time vehicle poses)来生成运动轨迹(trajectory)。最后，再将生成的运动轨迹传输至控制系统（Feedback Control），由其结合车辆状态（Vehicle State）不断纠正所规划的运动在执行过程中的偏差，并输出转向、油门和刹车（Steering, throttle, and break commands）指令。

![图片](Aspose.Words.f368d683-7bd0-4d9b-9cf6-d1b104590435.002.png)

​图7.2 规划和控制模块结构.（Destination：目的地；Route Planning：路径规划；Road network data：道路网络数据；Sequence of waypoints：路径点集序列；Behavioral Planning：行为决策；Traffic rules and perceived obstacles：交通规则和感知障碍物；Motion specification：运动规范；Real-time vehicle poses：实时车辆姿势；Motion Planning：运动规划；trajectory：运动轨迹；Vehicle state：车辆状态；Feedback Control；Steering, throttle, and break commands；转向、油门和刹车指令）

在这一章中，我们将深入研究规划控制模块，并介绍其中的路径规划算法，行为决策算法，以及反馈控制算法。同时，我们还介绍了一个关于 Apollo的迭代式最大期望规划算法（EM planner）的实际案例的研究，此算法是基于L4乘用车的自动驾驶而开发的。此外，我们介绍了PerceptIn的规划和控制框架，该框架是针对特定环境中实现低速自动驾驶而开发的，例如大学校园，游乐园，工业园区等。

**7.2 路径规划**

第一个子模块是路径规划模块，该模块通过校验地图上的道路网络信息，选择一条最佳的路径。具体来说，将道路网络视为一个有向图，每条边的权重代表穿越该路段的成本，那么寻找最佳路线的问题就可以表述为在道路网络图上寻找一条成本最低的路径。

**7.2.1有向加权图**

根据定义，一个有向图是一个有序的二元组 G=（V,E），其中V是G中顶点的集合，E是有序的顶点对的集合，这种通常被称为有向边。它与无向图不同，后者通常是以无序的顶点对来定义的，这些顶点对通常被称为边。图7.3中所示的是一个加权有向图的例子。

![图片](Aspose.Words.f368d683-7bd0-4d9b-9cf6-d1b104590435.003.png)

图7.3 加权图数据结构.

加权有向图也可以用来表示道路网络。例如，一个顶点可以表示为旧金山，另一个顶点可以表示为纽约，连接这两个顶点的边记录了这两个城市之间距离。那么，在此道路网络上，路径算法可以寻找两个城市之间的最短路线。

**7.2.2迪杰斯特拉（Dijkstra）算法**

首先，我们将在这里介绍第一种求解最短路径的算法：迪杰斯特拉（**Dijkstra**）算法[3]。该算法将从起始点开始，每次遍历周围未被访问的节点，并找到其中距离最短的点，然后将其加入到被访问的集合中。它从起点逐渐向外扩展，直至到达目标点。

![图片](Aspose.Words.f368d683-7bd0-4d9b-9cf6-d1b104590435.004.png)

​图7.4 Dijkstra算法的伪代码

详细情况如图7.4所示，迪杰斯特拉（**Dijkstra**）算法会迭代地计算从起始点到每个节点的最短距离，在每次迭代时会排除更长的距离。该算法主要由以下步骤组成：

1. 设置所有节点的初始距离为 "无穷大"；起始节点的初始距离为0。
1. 将起始点的距离值标记为永久的，其他节点的距离值标记为临时的。
1. 将起始节点设置为活动节点。
1. 将活动节点的距离值与各相邻边的权重相加，计算所有相邻节点到起始点的临时距离。
1. 若步骤4计算出来的某个节点的距离小于该节点当前的距离值，就更新这个距离，并将当前活动节点设为该节点的前置节点。 这个步骤也被称为"更新"，是迪杰斯特拉（**Dijkstra**）算法的核心思想。
1. 将具有最小临时距离的节点设置为活动节点，将其距离标记为永久的。
1. 重复步骤4-7直到所有节点的距离都被标记为永久。

**7.2.3 A\*算法**

虽然迪杰斯特拉（**Dijkstra**）算法能够保证找到一条最短的路径，但是当地图较大时，该算法的计算成本可能会非常高。有一种更快的算法，叫做贪婪最佳优先搜索算法，其原理与迪杰斯特拉（**Dijkstra**）算法类似。两者的不同之处在于：与迪杰斯特拉（**Dijkstra**）算法每次选择最接近起始点的节点的方法不同，此算法每次选择最接近目标点的节点。然而由于该算法依赖于启发式函数，所以并不能保证找到最短路径。

在本小节中，我们将介绍A\*算法，该算法是迪杰斯特拉（**Dijkstra**）算法和贪婪最佳优先搜索算法[4]的结合 。不仅能够像迪杰斯特拉（Dijkstra）算法一样寻找到最短路径，也能像贪婪最佳优先搜索算法一样使用启发式函数来进行引导。

迪杰斯特拉（**Dijkstra**）算法浪费了很多时间进行非必要的路径探索。贪婪最佳优先搜索算法在最有可能是最短路径的方向上进行探索，但它不一定能够找到最短路径。如图7.5所示，A\*算法同时计算从起点开始的实际距离和到达目标的预测距离。关于A\*算法的详细介绍可以参考[5]。

![图片](Aspose.Words.f368d683-7bd0-4d9b-9cf6-d1b104590435.005.png)

图7.5 A\*算法的伪代码

**7.3 行为决策**

在获得路径规划后，自动驾驶车辆须要能够遵循选定的路线进行行驶，并在行驶过程中遵循交通规则。根据感知到的其他车辆运动状况，道路状况和信号灯状态，行为决策模块负责将在该路径上规划合适的驾驶行为。

例如，当一辆自动驾驶汽车停在十字路口的停车线时，行为决策模块将会先发出停车的指令，并随后观察路口的交通状况，在发现自身可以通过十字路口后，则会命令车辆继续行驶。

然而，现实中的驾驶情况较为复杂，尤其是在城市环境中，预测其他车辆和行人的意图是一种很困难的行为。目前，业界对此类车辆和行人意图的预测问题进行了深入研究。并提出了一些基于机器学习的解决方法，例如高斯混合模型（GMMs）[6]。

对于其他未被观测到的交通参与者行为不确定性的问题（例如由GMMs产生的），在行为决策层中通常会使用概率决策模型来模拟。例如有限状态马尔科夫决策过程（MDPs）。部分可观察马尔可夫决策过程（POMDP）框架可以用来模拟未观测到的驾驶场景和行人意图并进行显式建模，并产生特定的近似解决策略[7]。

具体来说，POMDP算法是MDP的一个泛化。POMDP模拟一个代理决策过程，在这个过程中，假定系统状态的变化是由MDP决定的，但这个代理决策不能直接观察到当前的状态。相反，它必须根据一组观察值和观察概率以及底层MDP，形成一个所有可能状态的概率分布。

**7.3.1 马尔科夫决策过程（MDP）**

当面临决策时，会有许多不同的行动可供选择，每个行动都会导致不同的结果。最佳行动的选取需要考虑的不仅仅是该行动带来的即时奖励，还需要从全局的角度考虑长远的收益。即时奖励往往很容易观察到，然而长远的收益却并不一定那么明显。有些即时奖励不佳的行动，从长远来看可能会具有更好的效果。

MDP算法可用于对决策过程进行建模，以便我们模拟此决策过程。在MDP建模后，可以使用多种算法对该决策问题进行求解。MDP模型的四个元素分别是：状态集合、行动集合、状态转移矩阵、以及即时奖励。

S:状态集合。状态是对当前环境的描述，在智能体做出动作后，状态会发生变化。MDP的状态集合包含了环境中所有可能存在的状态。

A:行动集合。在每种状态下可以进行的所有行动的集合。决策过程的关键是要了解在特定的状态下，应当采取哪种行动。

T:状态转移矩阵。状态转移矩阵定义了每种行动如何改变当前的状态。在不同的状态下，同一个行动可以有不同的效果，所以我们需要在MDP中的为每种状态下执行的每种行动定义效果（执行行动后从当前状态转变为另一种状态），并且可以为每种行动设定概率。

R:即时奖励。如何我们想使决策过程自动化，那么我们必须衡量每个行动的效果，这样我们就可以比较不同行动的收益。因此，我们将指定在每个状态下执行不同行动的即时奖励。

我们称MDP的解决方案为策略，它指定了在每个状态下采取的最佳行动。我们需要一个价值函数来优化MDP的决策过程，以获得最终策略。该价值函数为每个状态定义了一个数值。因此，通过MDP模型，我们就得到了以下的信息参数：一组状态、一组可供选择的行动、一个即时奖励函数和一个状态转移矩阵。总而言之，以上这些算法的目标就是推导出一个从状态到行动的映射，它代表了自动驾驶车辆或智能机器人在每个状态下采取的最佳行动。

**7.3.2 值迭代算法**

如图7.6所示，值迭代算法是一种计算最优MDP策略及其价值的方法[8]。值迭代算法通过不停地迭代形成一系列的价值函数来求解最优价值函数。

值迭代算法首先试图找到步长为1的价值函数。由于我们只需要做出一次决策，所以此函数值等于各个状态的值。回顾一下，在MDP模型中，我们定义了即时奖励，它指定了每个行动在每个状态下的优劣程度。由于我们的步长为1，我们可以简单地通过即时奖励选择出每种状态下具有最高收益的行动。

接下来，迭代算法的第二步，需要确定步长为2的价值函数。由于一共需要做出两轮决策，所以行动的价值将是本轮采取的行动的即时奖励，加上下一轮采取的行动的奖励。由于我们先前已经得到了步长为1的价值函数，我们只需将每种可能发生的行为的即时奖励添加到第一轮获得的价值函数中，就能获得这两轮决策的最高收益，并求得步长为2的价值函数。

![图片](Aspose.Words.f368d683-7bd0-4d9b-9cf6-d1b104590435.006.png)

图7.6 价值迭代算法的伪代码

之后，算法将再次迭代；它将使用已经求得的步长为2的价值函数来计算出步长为3的价值函数。如此反复迭代直到求得我们所需的价值函数。

**7.3.3部分可见马尔可夫决策过程（POMDP）**

MDP和POMDP算法的主要区别在于能否观察到过程的当前状态。在POMDP算法中，我们在模型中加入了一组观察值。该观察值并不直接提供当前的状态信息，而是提供了关于状态的提示信息。

由于观察可以是概率性的，因此需要设定一个观察模型。这个观察模型提供在每个状态下获取不同观察值的概率。由于无法直接获取当前的状态，所以每一步决策都需要依据整个过程中所记录的历史信息。具体来说，某一特定时间点的历史信息是指，从整个决策过程开始到该时间点为止，先前所采取的所有行为以及获取的所有观察值。

记录所有状态的概率分布能够为我们提供整个决策过程中完整的历史信息。在POMDP中，我们必须记录每个状态的概率分布。当我们执行一个行动并获取观察值时，必须根据状态转移矩阵和观察值的概率对此概率分布进行更新。

如图7.7所示，一个POMDP由以下的内容定义：

![图片](Aspose.Words.f368d683-7bd0-4d9b-9cf6-d1b104590435.007.png)

​	图7.7 POMDP的示例

- S,状态集合
- A,行动集合
- O,观察集合
- P(S0),起始状态的概率分布
- P*(*S′|S, A),状态转移模型，定义了从状态S做动作A到达状态S′的概率
- R(S,A,S'),奖励函数，定义了从状态S做动作A到达状态S'的预期回报
- P(O|S)  定义了在状态S时观察值为O的概率

**7.3.4 求解POMDP**

在求解POMDP之前，我们首先需要了解“置信状态”的概念，它表示所有可能的模型状态的概率分布。假设目前只有0和1两种可能的状态，在t时刻的置信状态Pr(s = 0) = 0.75和Pr(s = 1) = 0.25。

如果我们已知"t"时刻的置信状态b，我们执行了动作"a"并获得了观察结果"z"，我们可以通过贝叶斯法则，运用下列公式来求得"t+1"时刻的置信状态，其中，S=状态集合，A=行动集合，Z=观察集合

![图片](Aspose.Words.f368d683-7bd0-4d9b-9cf6-d1b104590435.008.png)

![图片](Aspose.Words.f368d683-7bd0-4d9b-9cf6-d1b104590435.009.png)


然后，我们可以从任意给定的置信状态中准确地求得预期收益。对于每个置信状态，都能够求得一个单一的预期收益。通过计算每个置信状态下的预期收益，就可以获得定义在置信状态空间上的价值函数。接下来运用图7.6中的值迭代算法，就能够对POMDP进行求解。如果有对POMDP算法、论文、实例和源码感兴趣的读者，可以访问www.pomdp.org以获取更多的信息。

**7.4 运动规划**

行为决策层决定了在当前环境状态下需要执行的驾驶行为，例如，在车道内巡航，改变车道或者右转等。这些行为必须要转化为下一层的反馈控制器可以执行的运动轨迹。

简而言之，运动规划系统的主要任务是，在满足车辆动力学的前提下，生成一条能够避开传感器所检测到的障碍物，并且尽可能使乘客舒适的运动轨迹。

在大多数情况下，求得运动规划问题的精确解是比较困难的。因此，在实际情况中通常会使用数值近似法[1]。其中最常用的几种方法是将问题设定为函数空间中的非线性优化问题的变分方法(www.pomdp.org)，以构建车辆状态空间的离散化图并搜索最短路径的图搜索方法[9]，以及基于树的增量方法，该方法从车辆的初始状态开始，逐步探索下一步可以到达的状态，构建一棵包含所有可到达状态的树，然后选择该树的最佳分支。

在本节中，我们将介绍快速扩展随机树(RRT)和RRT\*算法，这两种都是在运动规划中广泛使用的基于树的增量方法。

**7.4.1 快速扩展随机树**

如图7.8所示，RRT是一种通过不断在空间中随机生成树节点，来高效搜索多维非凸空间的算法。该树是由搜索空间中随机抽取的样本逐步构建的，并且不断地向更大的未搜索区域扩展。RRT也可以被看作是一种具有状态约束的非线性系统生成开环轨迹的技术。对RRT算法的详细讨论可以参考[11]。

​	![图片](Aspose.Words.f368d683-7bd0-4d9b-9cf6-d1b104590435.010.png)

图7.8 RRT算法示意图

如图7.9所示，RRT算法以一个初始点为根节点，通过随机采样增加叶子节点的方式，生成一个随机扩展树。每个新的状态节点会尝试与树中最近的节点连接。如果连接可行，则会将新状态添加到树中。

通过对搜索空间的均匀采样，RRT树会优先向更大的未搜索的区域扩展。树与新的状态节点之间的连接长度通常会受到生长因子(距离的极限值)的制约。如果随机样本与树中最近的状态节点的距离超过了这个极限值，则树会连接与随机样本连线上的最大距离处一个新状态结点，而不是随机样本本身。

![图片](Aspose.Words.f368d683-7bd0-4d9b-9cf6-d1b104590435.011.png)

图7.9 RRT算法的伪代码

RRT算法的伪代码中提到，随机样本的探索方向可以被看作是树的扩展方向，而生长因子则决定扩展速度。这使得在限制了RRT增长速度的同时，保持了RRT的空间填充偏差。

需要注意的是，如果增加某个特定区域采样的概率，RRT的增长可能会产生偏差。大多数RRT的实际应用都会利用这一点来引导RRT向着规划的目标方向进行搜索。主要方法是，在状态抽样过程中，略微增加目标方向的采样概率，这个概率越高，树越容易向着目标生长。

**7.4.2 RRT\*算法**

尽管RRT算法在实践中是有效的，并且具有概率完备性等理论保证，但是RRT算法已经被证实可能无法收敛到最佳值。RRT\*是RRT的改进版本，它被证明是渐进最优的算法，因此它返回的解决方案一定会收敛到最优[9]。

如图7.10所示，RRT\*的基本原理与RRT相同，但是算法中增加了两个关键部分导致其产生了截然不同的结果[12]。 首先，RRT\*算法记录每个节点相对于其父节点的距离，当作节点代价。在图中找到最近的节点后，检查以新节点为圆心的固定半径内的邻域节点，如果发现一个比当前最近节点低的节点，那么该节点将会取代当前最近节点。这部分的效果可以通过树形结构中增加的扇形树枝看出来。此方法使得RRT的立体结构被消除了。

RRT\*增加的第二个关键部分是对随机树进行重新布线。当树连接了一个新节点之后，新节点的邻居节点会被再次检查，如果将邻居节点的父节点改为新添加的节点会使其路径成本降低，则将邻居节点的父节点更改为新节点。这一特性使得路径更加平滑。

![图片](Aspose.Words.f368d683-7bd0-4d9b-9cf6-d1b104590435.012.png)

图7.10 RRT\*算法的伪代码

**7.5 反馈控制**

为了让车辆执行运动规划系统的输出的路径轨迹，反馈控制器将会给执行器提供合适的输入，以执行规划的运动轨迹并实时纠正跟踪误差。在执行过程中产生的跟踪误差，有一部分是由于车辆模型的不精确性所导致的。

如图7.11所示，反馈控制器的作用是当存在建模误差和其他形式的未知误差时，尽可能地减小实际运行轨迹与参考轨迹的偏差。具体来说，为了尽可能地消除该误差，控制器(Controller)将计算系统输出(System output)与参考值(Reference)的测量误差(Measured error)作为输入，并通过控制器进行适当调节，然后生成一个新的系统输入(Systen input)。

![图片](Aspose.Words.f368d683-7bd0-4d9b-9cf6-d1b104590435.013.png)

图7.11反馈控制（Reference：参考值；Measured error：测量误差；Controller：控制器；System input：系统输入；System：计算系统；System output：系统输出；Sensor：传感器；Measured output：测量输出）

**7.5.1比例积分微分(PID)控制器**

比例-积分-导数（PID）控制是一种广泛用于自动驾驶的反馈控制方法[13]。如图7.12所示，PID控制器通过比例（P）、积分（I）和微分（D）过程不断修正实际输出与期望输出之间的误差。

![图片](Aspose.Words.f368d683-7bd0-4d9b-9cf6-d1b104590435.014.png)

图7.12 PID控制器

汽车上的巡航控制是一个常见的PID控制。在上坡时，如果发动机的功率恒定，那么车辆的速度会降低，PID控制通过增加发动机的输出功率，保证超调量尽可能小的情况下，在最短的时间内，使得车辆能够达到期望的速度。

PID控制运用比例、积分和微分三个控制项调节控制的误差，以获得最佳控制。具体来说，比例控制就是用常数Kp（比例常数）与当前误差相乘；积分控制考虑过去的误差，将误差值过去一段时间的总和乘以一个常数Ki（积分常数）；微分控制考虑将来的误差，计算误差的一阶导，并和一个常数Kd相乘（微分常数）。三者通过线性组合形成控制量，对被控对象进行控制。PID的伪代码如图7.13所示。



![图片](Aspose.Words.f368d683-7bd0-4d9b-9cf6-d1b104590435.015.png)

图7.13 PID伪代码

**7.5.2模型预测控制（MPC）**

模型预测控制(MPC)是一种进阶过程控制方法，可以在各类约束的条件下实现系统性能的最优控制[14,15]。如图7.14所示，模型预测控制（MPC）的实现依赖于过程的动态模型（通常为系统识别获得的线性经验模型）。MPC根据观测到的系统当前状态(measurements)，动态模型(process)，期望的输出值(reference)，通过基于模型的最优控制算法(model-based optimizer)求解获得最佳的控制输入(input)。MPC的主要优点是在优化过程中同时考虑当前时间段和未来时间段的效果。相比PID控制器而言，MPC的优点是可以预测未来事件并进行相应的处理。

![图片](Aspose.Words.f368d683-7bd0-4d9b-9cf6-d1b104590435.016.png)

图7.14 模型预测控制（reference：参考值；model-based optimizer：基于模型的最优控制算法；input：输入；process：动态模型；measurements：测量值；output：输出）

在[16]中可以找到一个将MPC应用于自动驾驶车辆的详细示例，该示例主要包含以下部分：

- 车辆控制接口：车辆每100毫秒调整一次转向、油门和刹车的控制动作。
- 代价函数。从宏观上看，代价函数表示目标轨迹路径与车辆实际的行驶轨迹之间的差异。从微观上看，代价函数是航迹误差、航向误差、速度代价、转向代价、加速代价、转向率变化和加速度变化的加权和。
- 限制条件：车轮转角不能超过25°。

在此案例中，MPC可以在满足约束条件的情况下求解代价函数的最小值。MPC控制器每个周期(100ms)都会通过传感器读取车辆位置(x, y)、速度v、航向角ψ、转向角δ和加速度a等数据，以确定车辆的当前状态。

随后MPC控制根据当前车辆的状态，生成未来短时间内(例如1秒)车辆的最优动作。

例如，假设MPC生成的最优计划是先顺时针旋转方向盘20°，随后每100ms减少1°，根据计算这个动作能够在一段时间(1s)后获得最小的代价函数。

随后MPC将会进行第一个动作，将方向盘顺时针旋转20°。在下一个控制周期（100ms后），MPC将再次读取传感器的输入，不再继续执行先前的动作，而是重新计算下一个最优动作。随后的每个控制周期(100ms)都不断重复此过程。

MPC的优点在于，它不是简单地只考虑当前状态的生成单步决策，而是结合了未来的系统状态，反复规划下一步的动作。例如，在此案例中提前规划了未来1秒钟(10个控制周期)的动作。与PID不同，MPC更不易受到贪心算法中短视收益的影响，因此能够获得更平滑的运动轨迹。

下面是基于MPC解决此问题的具体步骤：

1. 读取当前的车辆状态，包括位置（x，y），速度v，航向ψ，转向角δ，以及加速度*a*。
1. 通过最优控制算法，在给定的约束条件下，求解代价函数的最小值，生成接下来的10个周期（每周期100毫秒）的油门、刹车和转向的最优控制动作。
1. 只执行第1个周期的油门、刹车和转向控制动作。
1. 返回到第1步。

**7.6 Apollo中的EM Planning 迭代系统**

在前几节中，我们介绍了路径、行为和运动规划的基本知识；在本节中，我们将介绍一个开源的规划控制系统，即Apollo的EM Planner 迭代系统[17]。通常情况下，规划的轨迹应当被表示为一连串“轨迹点”，其中每个点都包含了当前的位置、时间、速度、曲率等信息。在我们深入了解Apollo自动驾驶系统的细节之前，有必要对其中的一些重要术语进行说明。

**7.6.1 术语**

**7.6.1.1 路径和轨迹**

路径(path)表示行驶的路线，其中包含位置、曲率和相对于弧线的曲率导数的信息，通常由一系列表示其形状的“路径点(waypoints)”组成。然而，路径只描述了路线的形状，并不包含速度和时间的信息。

相比之下，轨迹(trajectory)包含了路径(path)的所有信息，以及沿着该路径的速度信息。图7.15给出了关于路径和轨迹的数据定义的详细信息。

![图片](Aspose.Words.f368d683-7bd0-4d9b-9cf6-d1b104590435.017.png)

图7.15 路径和轨迹的定义

**7.6.1.2 SL坐标系和参考线**

自动驾驶车辆的运动规划必须符合“道路结构”，这是规划的一个重要条件。由于自动驾驶车辆是在结构化的道路上运行，而不是在自由空间上运行，因此Apollo中的规划模块必须在高清地图指定的道路坐标中进行路线规划。

具体来说，道路坐标系由一条参考线表示。令参考线的方向为 "s "方向，垂直于参考线的方向为 "l "方向。将大地坐标系称作"cartesian"坐标系，将参考线的SL坐标系称作"Frenet"坐标系[18]。

给定一条预定的参考线，其中的任意一个点在cartesian坐标系(x,y)和Frenet坐标系(s,l)存在双向映射(能通过在cartesian坐标系的坐标x,y，求得在Frenet坐标下的s,l，反之亦可)。在Frenet坐标系下更容易进行语义描述，例如，在Frenet坐标系中很容易表示周围车辆的信息，而在cartesian坐标系中很难清晰地描绘出这样的语义对象，因此选用Frenet坐标系进行路径规划。

**7.6.1.3 ST图**

在SL坐标系下，车辆沿着参考路径运动时，其速度与时间的关系可以用ST图来表示，坐标轴x表示时间T，y轴表示路径方向的距离S。下面我们将通过图7.16的例子来对ST图做一个详细的阐述，以帮助理解。在图7.16（b）中，用红车表示本车，在t时刻下将要在一个路口进行右转。同时我们可以发现，车辆2在本车将要驶入的车道上行驶，车辆1正要直行穿过路口。

如果本车的运动规划是给车辆2让行（Yield），同时超过（Overtake）车辆1，则规划的ST图会如图7.16(a)所示。ST图中1和2之间的虚线轨迹就是本车的速度曲线。ST图是一个简单却十分实用的工具，能够帮助我们计算参考路径下的速度曲线。

![图片](Aspose.Words.f368d683-7bd0-4d9b-9cf6-d1b104590435.018.png)

图7.16 ST图示例（start point：起始点；Yield：让行；Overtake：超车）

**7.6.2 EM Planning 迭代算法**

有了SL坐标系和参考线的概念后，Apollo的规划算法将按照如图7.17所示的方式，在(S,L,T)坐标系中迭代进行。

![图片](Aspose.Words.f368d683-7bd0-4d9b-9cf6-d1b104590435.019.png)

图7.17 EM Planning迭代方式（Traffic Decider：交通决策模块；DP Path：动态规划路径；Path Decider：轨迹决策模块；DP Speed：动态规划速度；Speed Decider：速度决策模块；QP Path：二次规划路径；QP Speed：二次规划速度）

整个优化过程的第一步是 通过动态规划(DP)算法获得DP Path和DP Speed, 该算法的输出会生成并优化路径(s, l)，然后生成并优化速度曲线(s, t)。

由动态规划(DP)获得的路径(DP path)和速度曲线(DP speed)由轨迹决策模块(Path Decider)和速度决策模块(Speed Decider)解析并存储。接下来将进行优化过程的第二步——二次规划(QP)。二次规划(QP)将在动态规划(DP)的基础上进一步优化，以获得最优解决方案（QP Path 和 QP Speed）。

二次规划(QP)的约束条件是由Path Decider和Speed Decider解析DP path和DP Speed后给出的。经过两轮的迭代后（DP+QP），将最终规划的轨迹输出给下游的控制模块。接下来，我们将对整个迭代过程做出更详细的介绍。

**7.6.2.1 交通决策**

交通决策模块负责制定“交通规则"。此处的交通规则指的是根据实际交通法规编制的硬编码规则(hard‐coded rules)。举个简单的例子，如果前方存在一个停车标识牌或者是人行横道，那么交通决策模块将从高清地图中检索出一条停车线，并将停车的位置传递给下层模块。

下面介绍一种实现这种硬编码的停车规则的方法。我们可以在此停车线的位置创建一个"虚拟对象"，然后传递给轨迹和速度决策模块，后者生成的轨迹将会停留在此"虚拟对象"之后。下层的路径和速度规划必须遵守交通决策模块给出的约束条件。

在交通决策模块给出约束条件后，将进行优化过程的第一步——动态规划(DP)。动态规划的第一步是沿着(s,l)坐标系采样。对于相邻层次的每两个点 (s1, s1′ = 0, s1″ = 0)和(s2, s2′ = 0, s2″ = 0)，我们可以拟合一个五次多项式来进行连接。随后，我们采用光滑的多项式螺旋线将这些采样点逐层连接起来。各层之间的每条曲线都有不同的代价。图7.18 展示了EM Planner中路径的动态规划(DP)的示例。

![图片](Aspose.Words.f368d683-7bd0-4d9b-9cf6-d1b104590435.020.png)

图7.18 迭代EM规划算法中基于DP的路径规划（ADC：自动驾驶车辆）

为了表述这一点，用A(i, j)表示第j层第i个点，函数Cost(A(i, j))表示连接到第j层第i个点的所有路径的最小累积代价。那么Cost(A(i, j))可以表示为:

![图片](Aspose.Words.f368d683-7bd0-4d9b-9cf6-d1b104590435.021.png)

连接到特定点的最优路径代价可以通过上述递归公式表示，可以用经典的动态规划(DP)方法来进行求解。每条螺旋线的成本代价需要考虑到以下两个方面：(i)到中心参考线的累计横向距离；(ii)静态障碍物的躲避。假设每层有n个采样点，在横向方向共有m层，动态规划(DP)算法可以在O(nm^2^)的时间内找到最优解。

路径动态规划(DP path)算法求得的路径只是一个粗略的解。在此阶段因为没有对速度相关信息进行计算，所以只考虑了静态障碍物。

速度动态规划(DP speed)的解法与路径动态规划(DP path)类似，唯一的区别是求解的空间从(s,l)坐标系变成了(s,t)坐标系。速度动态规划(DP speed)算法是在一个基于(s, t)坐标系的网格中进行的，其中部分网格被动态障碍物的投影所占据。该算法的最终目标也是寻找一系列的由多项式螺旋线连接的采样(s,t)点，来获取到达某个期望位置的最优解。

如图7.19所示，路径动态规划(DP path)和速度动态规划(DP speed)获得的结果将由路径和速度的决策模块(path and speed decider)解析后做出决策。从图中可以看出，车辆可以通过“左侧绕行”、“右侧绕行”或“停车”的动作来躲避静态障碍物，通过“让行”或“超车”的动作来躲避动态障碍物。

![图片](Aspose.Words.f368d683-7bd0-4d9b-9cf6-d1b104590435.022.png)

图7.19 根据DP path and speed的计算结果可以做出的决策（ADC：自动驾驶车辆；Left Nudge：左侧绕行；Right Nudge：右侧绕行；Stop：停车；Yield：让行；Overtake：超车）

**7.6.2.2 二次规划(QP)轨迹和速度**

尽管通过动态规划(DP)能够规划出一条安全、无碰撞的路径，但是并不能够保证该路径是平滑的。

二次规划(QP)通过将代价函数转化为多个变量的二次函数，在给定的约束条件的下，求解该二次函数的最优解，在此处可以理解为求解出一条最优且足够平滑的路径。下面我们将详细介绍如何求解二次规划轨迹(QP path)的问题。求解二次规划速度(QP speed)问题与其相似，感兴趣的读者可以参考Apollo 2.0的QP部分的程序说明文档[17]。

在QP path的问题中，l，s的关系可以通过以下函数关系表述:

![图片](Aspose.Words.f368d683-7bd0-4d9b-9cf6-d1b104590435.023.png)

需要优化的代价函数是f(s)及其高阶导数的累加值:

![图片](Aspose.Words.f368d683-7bd0-4d9b-9cf6-d1b104590435.024.png)

QP问题中的等式约束包含两个方面。首先,起始点必须是固定的:

![图片](Aspose.Words.f368d683-7bd0-4d9b-9cf6-d1b104590435.025.png)

为了满足平滑性的要求，从起点到目标点过程中的各个点，形成的曲线必须是连续的，可以表述为:

![图片](Aspose.Words.f368d683-7bd0-4d9b-9cf6-d1b104590435.026.png)

上一步动态规划(DP)的已经规划了躲避静态障碍物的动作，比如“左侧绕行”、“右侧绕行”或“停车”。DP路径中的障碍物对象在QP问题中可以作为一种不等式约束。

更具体地来说，在某个的s值下，存在一个障碍物，那么在当前可以取得的l值应当处于车道线边界和障碍物边界之间，这样就形成了QP问题的不等式约束。根据上述等式约束和不等式约束，整个QP path的优化问题可以简化以下形式的经典QP问题：

![图片](Aspose.Words.f368d683-7bd0-4d9b-9cf6-d1b104590435.027.png)

![图片](Aspose.Words.f368d683-7bd0-4d9b-9cf6-d1b104590435.028.png)

![图片](Aspose.Words.f368d683-7bd0-4d9b-9cf6-d1b104590435.029.png)

![图片](Aspose.Words.f368d683-7bd0-4d9b-9cf6-d1b104590435.030.png)

以![图片](Aspose.Words.f368d683-7bd0-4d9b-9cf6-d1b104590435.031.png)部分为例展开说明，该部分在QP代价函数中可以写作:

![图片](Aspose.Words.f368d683-7bd0-4d9b-9cf6-d1b104590435.032.png)



![图片](Aspose.Words.f368d683-7bd0-4d9b-9cf6-d1b104590435.033.png)

通过二次规划(QP)算法，车辆能够规划出一条足够平滑的轨迹。速度的二次规划(QP speed)与轨迹的二次规划(QP path)的方法基本一致，在（s,t)坐标系下对先前 DP speed获得的速度曲线进行优化。在DP和QP过程中，都是先求出(s,l)坐标系下的轨迹，然后再根据该轨迹优化(s,t)坐标系下的速度曲线。该过程类似于EM算法通过极大似然估计，求解未知参数的过程，因而将此规划过程称为EM 轨迹规划。

**7.7 Perceptln规划控制框架**

在本节中，我们将介绍PerceptIn的规划和控制框架，该框架是针对特定环境工作情况况下实现低速自动驾驶而开发的，例如大学校园，游乐园，工业园区等。

图7.20是PerceptIn的规划和控制框架具体结构，通过上层模块输入的路径、定位、感知等信息(import route info, localization and perception, etc)，对车辆进行规划控制，输出转向、油门、刹车指令(steering, throttle and brake commands)。其中包括任务规划模块(mission planner)，行为规划模块(behavior planner)，运动规划模块(motion planner)和车辆控制模块(vehicle controller)：

任务规划模块(mission planner)：任务规划模块定义了两个基本任务。"A→B "定义了从任意点A到任意点B的任务,“停车(parking)”定义了车辆在指定位置自行停车的任务。在可控的环境下，这两个简单的任务已经足够了。因为在大多数情况下，人们只是希望自动驾驶车辆能够从A点到达B点，同时也希望在需要时能自行停车。

行为规划模块(behavior planner)：行为规划模块则定义了为了完成一项任务，而发生的所有可能的行为。一旦任务规划模块定义了一项任务，就会为车辆规划一条行驶的车道。最理想的情况是能够让车辆沿着车道行驶(lane keeping)，或让车辆停留在指定的车道上。在该车道上，如果有车辆在本车的前方行驶，且相对速度接近于零(obstacle relative speed is about zero),那么我们可以选择"car follow"(跟车)行为，跟随前车行驶。如果检测到前方有静态障碍物或低速障碍物(static obstacle or low speed)，那么我们可以选择 “avoidance” (避让)行为，减速并绕过障碍物。一旦车辆到达目的后(reach goal)，我们就从"lane keeping"(车道保持)状态转换至“mission\_complete” (任务完成)状态，随后停下车辆。

运动规划模块(motion planner)：为了实现行为规划模块中定义的行为，运动规划模块定义了多种运动方式。首先是"attach lane"(车道附着)，顾名思义，就是车辆保持在原车道上。如果我们探测到车辆远离了车道(far form lane)，那么我们将通过"closer to lane"(靠近车道)行为来使车辆回到指定的车道上。如果当前车辆将要转弯，无论左转还是右转，我们将进行 “turning” (转向)行为。当到达目的地或是出现了其他需要停车的情况(expected or unexpected stop)，我们将通过"stop"(停车)行为停下当前车辆。

车辆控制模块(vehicle controller)：该模块被定义在最底层，运用MPC和PID混合控制方式(MPC和PID的介绍请参考本章7.5节)，将上层模块中规划的行为转化为对控制指令，通过输出转向、油门、刹车指令实现对车辆的控制。

![图片](Aspose.Words.f368d683-7bd0-4d9b-9cf6-d1b104590435.034.png)

图7.20 PerceptIn规划控制框架（import route info, localization and perception, etc.：输入路径、定位、感知等信息；mission planner：任务规划模块；ready to park：准备停车；parking：停车；behavior planner：行为规划模块；avoidance：避让；obstacle clear：障碍物消失；static obstacle or low speed：静态障碍物或低速障碍物；lane keeping：保持车道；vehicle clear：车辆消失；car follow：跟车；obstacle relative speed is about zero：与障碍物车辆的相对速度接近于零；reach goal：达到目标；mission\_complete：任务完成；motion planner：运动规划模块；closer to lane：靠近车道；attached lane：附着车道；attach lane：车道附着；far from lane：远离车道；turning finished：转向完成；turning：转向；turning node：转向节点；expected or unexpected stop：车主预期或意外停车；stop release：解除停车；Stop：停车；vehicle controller：车辆控制模块；hybrid MPC and PID：混合MPC和PID控制；steering, throttle, and brake commands：转向、油门和刹车指令）

PerceptIn规划控制框架生成控制指令的周期为100毫秒(10Hz)，将端到端（从感知和定位输入到控制输出）的计算延迟保持在50毫秒以内，以确保实时规划的安全与可靠。



**参考文献**

1 Paden, B., Čáp, M., Yong, S.Z. et al. (2016). A survey of motion planning and control techniques for self‐driving urban vehicles. *IEEE Transactions on Intelligent Vehicles* 1 (1): 33– 55. 

2 Liu, S., Li, L., Tang, J. et al. (2017). *Creating Autonomous Vehicle Systems* , Synthesis Lectures on Computer Science, 1– 186. Morgan & Claypool Publishers. 

3 Dijkstra, E.W. (1959). A note on two problems in connexion with graphs. *Numerische Mathematik* 1 (1): 269– 271. 

4 Hart, P.E., Nilsson, N.J., and Raphael, B. (1968). A formal basis for the heuristic determination of minimum cost paths. *IEEE Transactions on Systems Science and Cybernetics* 4 (2): 100– 107. 

5 Red Blob Games (2014). Introduction to the A\* Algorithm. https://www.redblobgames. com/pathfinding/a‐star/introduction.html (accessed 1 June 2019). 

6 Havlak, F. and Campbell, M. (2013). Discrete and continuous, probabilistic anticipation for autonomous robots in urban environments. *IEEE Transactions on Robotics* 30 (2): 461– 474. 

7 Ulbrich, S. and Maurer, M. (2013). Probabilistic online POMDP decision making for lane changes in fully automated driving. In: *16th International IEEE Conference on Intelligent Transportation Systems (ITSC 2013)* , 2063– 2067. IEEE. 

8 Poole, D.L. and Mackworth, A.K. (2010). *Artificial Intelligence: Foundations of Computational Agents* . Cambridge University Press. 

9 Karaman, S. and Frazzoli, E. (2011). Sampling‐based algorithms for optimal motion planning. *The International Journal of Robotics Research* 30 (7): 846– 894. 

10 LaValle, S.M. and Kuffner, J.J. Jr. (2001). Randomized kinodynamic planning. *The International Journal of Robotics Research* 20 (5): 378– 400. 

11 LaValle, S. (n.d.). The RRT Page. http://msl.cs.uiuc.edu/rrt/ (accessed 1 June 2019). 

12 Medium. Robotic Path Planning: RRT and RRT\*. https://medium.com/@theclassytim/ robotic‐path‐planning‐rrt‐and‐rrt‐212319121378 (accessed 1 June 2019). 

13 Marino, R., Scalzi, S., and Netto, M. (2011). Nested PID steering control for lane keeping in autonomous vehicles. *Control Engineering Practice* 19 (12): 1459– 1467. 

14 Garcia, C.E., Prett, D.M., and Morari, M. (1989). Model predictive control: theory and practice— a survey. Automatica 25 (3): 335– 348. 

15 Bemporad, A. Model Predictive Control. http://cse.lab.imtlucca.it/~bemporad/teaching/ ac/pdf/AC2‐10‐MPC.pdf (accessed 1 June 2019). 

16 Hui, J. (2018). Lane keeping in autonomous driving with Model Predictive Control & PID. https://medium.com/@jonathan\_hui/lane‐keeping‐in‐autonomous‐driving‐with‐modelpredictive‐control‐50f06e989bc9 (accessed 1 June 2019). 

17 GitHub. Apollo Auto. https://github.com/ApolloAuto (accessed 1 June 2019). 

18 Werling, M., Ziegler, J., Kammel, S., and Thrun, S. (2010). Optimal trajectory generation for dynamic street scenarios in a Frenet frame. In: *2010 IEEE International Conference on Robotics and Automation* , 987– 993. IEEE.

