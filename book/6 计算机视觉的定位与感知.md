**6 计算机视觉的定位与感知**

**6.1 简介**

如图6.1所示，计算机视觉在自动驾驶车辆和智能机器人的模块化设计方法中扮演了重要角色。它的主要功能是提供两个基本的能力：一是回答机器人在哪里的问题（定位），二是回答机器人周围存在什么的问题（感知）[1]。![图片](Aspose.Words.cc5469fa-e29d-48b0-91c7-260e90827e7f.001.jpeg)

图6.1 无人车模块化设计架构

从图6.1中我们可以看到，红方框所示部分是本章将要讲述的计算机视觉技术。其他组件包括底盘（Chassis）、声呐（Sonars）、毫米波雷达（Radar）、控制器区域网络（CAN bus）、规划控制（Planning and Control）、地图（Map）和全球定位系统（GNSS）。 接下来，让我们来了解本章的大致流程。首先在6.1节中，将回顾计算机视觉技术的一些细节。接着，在第6.2节中，从计算机视觉硬件设计开始，介绍在设计和构建计算机视觉硬件时所面临的挑战。在第6.3节中，将介绍相机校准的概念，并深入探讨一些比较流行的校准软件和技术。在第6.4节中，将解释如何使用计算机视觉实现自动驾驶车辆和智能机器人的定位。在第6.5节中，将介绍如何使用计算机视觉进行感知。最后，在6.6节中，将以PerceptIn公司的DragonFly无人驾驶车辆的计算机视觉模块为例，为读者提供学习和研究的实例。

在阅读完本章后，读者应该能够理解如何将计算机视觉的相关技术应用在无人驾驶车辆和智能机器人的设计中，同时可以了解到怎么样结合计算机视觉和其他传感器来实现更好的定位和感知效果。

**6.2 搭建计算机视觉硬件**

可以这样说，搭建计算机视觉硬件并利用摄像头等传感器执行感知和定位任务是极具挑战性的，因为在无人驾驶车辆中需要考虑许多设计因素。这些因素包括选择何种图像传感器、选择何种镜头，以及是在设备上进行集中式计算还是通过云端或者边缘进行计算。本节中，我们将详细讨论这些设计上带有挑战性的问题。

**6.2.1 七层技术**

如图6.2所示，如果我们需要搭建一个计算机视觉的硬件设备，这需要读者至少考虑七层技术的选择与使用。

![图片](Aspose.Words.cc5469fa-e29d-48b0-91c7-260e90827e7f.002.jpeg)

图6.2 构建计算机视觉硬件的七层技术

从上图中我们可以看到左边是详细的七层技术(Seven Layers of Technologies)，而右边是一个产品(One Product)的示意图。主要我们来看一下左边，首先最下面一层是镜头(Lens)，依次向上为:图像传感器(Image Sensing)、图像信号处理器(Image signal processor，ISP)、机械设计(Mechanical Design)、传感器融合(Sensor Fusion)、计算系统(Computing Systems)、算法(Algorithms)。

① 镜头(Lens)：在构建计算机视觉的硬件设备中，镜头的选择对于传感器所接收到的光线数量具有关键影响。在机器人视觉中，常使用最为简单的针孔模型，该模型通过留出一个小孔并阻挡大部分光线来限制所接收到的光线数量。在理想情况下，图像传感器上的每个像素都可以接收到物体上的一个点发出的射线。镜头的两个基本参数是焦距(focal length)和最大光圈(maximum aperture)。焦距决定了物体在图像平面上成像的放大率，而光圈则决定了图像的光强。焦距也决定了相机的视场角，通常来说，焦距越短，视场角越广。同时，更大的光圈可以在相同的曝光下提供更快的快门速度。因此，在设计计算机视觉系统时，需要根据实际需要选择合适的镜头参数。

② 图像传感器(Image Sensing)：图像传感器在七层计算机视觉技术中，它负责将原始图像转化为数字信号并传递给图像信号处理器进一步处理。图像传感器的工作原理是将光波转换为数字信号，这种光波可以是可见光，也可以是其他类型的电磁辐射。消费级相机产品通常使用互补金属氧化物半导体（CMOS）作为图像传感器，因为它比电荷耦合器件（CCD）更便宜，功耗更低，延长了电池供电的设备的使用时间。而高端相机，如卫星上的相机，通常会使用CCD传感器。此外，根据不同的使用场景，我们也会根据需求动态调整图像传感器。例如，在白天可以使用可见光传感器，而在夜晚则需要使用红外成像传感器，以提供更好的成像效果。图像传感器的性能对于自动驾驶的实现有着至关重要的影响，需要进行充分的测试和评估以确保其在各种不同的使用场景下均能够保持稳定的性能表现。

③ 图像信号处理器(ISP):  图像信号处理器(ISP)是相机中非常重要的组件，它的功能包括去马赛克、自动对焦、曝光和白平衡等。在相机中，图像传感器(Image Sensing)中的像素对某些波长之间的光非常敏感。然而，这也意味着图像传感器本身无法明确地区分不同的颜色。为了获得彩色图像，一般需要在图像传感器上放置彩色滤光片，常见的是拜尔式(Bayer pattern)彩色滤光片。然后，通过对相邻像素颜色进行差值计算，就可以完成图像颜色的过渡，从而获得高质量的彩色图像。因此，在不同的光照条件下，选择正确的图像信号处理器(ISP)及其参数非常重要，以确保能够准确地检测和处理图像中的几何特征。此外，相机的性能还受到图像传感器和图像信号处理器的综合影响，因此，对于不同的使用场景，需要根据实际需求进行动态调整，以实现最佳的成像效果。

④ 机械设计(Mechanical Design)：机械设计(Mechanical Design)在自动驾驶领域中扮演着非常重要的角色，尤其是当设备上装有多个传感器时（例如一组相机）。机械设计需要考虑如何将各个传感器的位置固定在一个合适的位置，以便于获取最佳的数据质量。同时，机械设计也需要考虑到设备在日常工作中会面临的各种环境因素，如振动、温度等，以确保传感器的刚性链接和相对位置的稳定性。一个优秀的机械设计可以尽量减少设备需要对传感器进行重新校准的需求，从而提高设备的稳定性和可靠性。此外，机械设计也需要考虑设备的易用性和维护性，以便于设备的维护和更新。

⑤ 传感器融合(Sensor Fusion)：。在面对需要多个传感器的场景时，传感器融合可以说是一个挑战，因为每个传感器的加入都会给系统的校准和同步带来更大的难度以及复杂性。为了解决这个问题，我们需要将不同传感器的数据在空间和时间上进行校准和对齐，以确保在使用融合后的数据之前，数据已经经过了充分的处理和预处理。

⑥ 计算系统(Computing Systems)：在实践中，计算机视觉算法的计算成本通常非常高，一般的处理器是不能够胜任这项工作的。因此，开发一个能够提供高帧率和高精度的计算系统是一项具有挑战性的工作。为了解决这个问题，现在有多种计算机视觉加速方法可供选择。比如并行处理、硬件加速或者将通过网络将复杂的计算转移到另一个具有更高算力的机器上等方法就可以有效的解决上述的问题。

⑦ 算法(Algorithms)：一旦我们拥有了设计好的传感器系统和计算系统，最后一步就需要决定在自己的计算机视觉系统上运行什么算法了。通常情况下，计算机视觉可以用于自动驾驶的感知和定位模块。对于定位模块，如果只使用视觉系统，一般会使用视觉同步定位和建图算法(VSLAM)来进行定位。如果使用了视觉系统和惯性测量单元(IMU)，那么可以设计一个视觉-惯性里程计(VIO)来完成车辆的定位。如果使用了视觉系统、IMU和全球导航卫星系统(GNSS)这三种传感器，那么可以将VIO与GNSS结合，组成VIO-GNSS的融合定位系统。其中，GNSS可以在信号良好的情况下，给出准确的真值数据。在感知方面，我们可以通过使用双目立体摄像头，基于两个摄像头生成的视差图来完成深度估计。此外，近年来越来越多的研究也在使用深度学习技术对图像进行分割，以获取所需的结果。

**6.2.2 硬件同步**

假设整个计算机视觉硬件使用了上文所说的传感器融合方案，这一小节我们将向读者展示如何同步将不同的硬件传感器进行同步。比如说，当我们需要通过双目立体视觉的视差来计算出物体的深度信息时，那么两个摄像头的图像必须进行实践同步。否则，如果说左边的图像相较于右边的图像有100毫秒的延迟，那么整个系统将没有办法计算出准确的深度信息，特别是将这个计算机视觉设备部署在移动的自动驾驶车辆上时。简而言之，随着系统中被增加越来越多的传感器，例如IMU、GNSS等，那系统设计的硬件同步设计将会变得越来越复杂。

如图6.3所示，整个硬件的延迟以及传输通信流程如下。图像传感器(Image Sensor)与图像信号处理器(ISP)之间的信号传输是通过移动工业处理器接口(MIPI)实现的，其中CMOS图像传感器到ISP以及ISP到CPU之间的延迟大约是10纳秒左右。IMU与CPU之间的信号传输一般会通过I2C接口完成，延迟也是在10纳秒左右。与IMU相比，我们可以看到图像传感器在ISP上还存在一个10毫秒左右的图像预处理过程，这个固件(firmware)处理的时间一般是固定的，在数据同步的阶段需要考虑这部分的延迟。此外，在CPU中还存在三个部分，分别是驱动程序(driver)、执行时间(runtime application)以及应用(application)。在整个计算机视觉系统中，这些部分的延迟都需要考虑在内，以保证整个系统的实时性和准确性。![图片](Aspose.Words.cc5469fa-e29d-48b0-91c7-260e90827e7f.003.jpeg)

图6.3 硬件时间同步流程

总体而言，越接近原始数据的位置完成传感器的时间同步，观测的传感器时间越准确，与真实时间的差异也越小。例如，有些硬件可以在图像传感器到图像信号处理器（ISP）之间完成时间同步，这将得到一个约为10纳秒的延迟，几乎可以忽略不计。然而，如果我们在CPU应用程序层面上使用程序完成时间同步，那么可能会出现高达100毫秒的延迟，这对于高速移动的车辆而言可能会是灾难性的。

因此，对于双目立体视觉摄像头来说，如果两个图像传感器都连接到同一个图像信号处理器(ISP)，那么有一个有效且简单的方法可以实现两个图像传感器之间的同步。这种方法是通过ISP来完成图像传感器之间的硬件同步信号的触发，并同时触发自动曝光和图像预处理等功能，以提高图像质量。通过这种方法，两个图像传感器可以同步捕捉到相同的场景，避免了在后期图像处理时出现的时间不一致的问题，从而提高了立体视觉系统的测量精度和图像质量。

**6.2.3 计算**

通过上述步骤，我们已经得到了传感器数据的时间同步集合。接下来，我们需要考虑如何对这些传感器数据进行计算，以得到我们所需的结果。一般而言，计算机视觉对于算力的要求极高，因此工程师需要在帧率和算力之间寻求平衡。如果使用商用的嵌入式处理器，很难实现高帧率（例如15帧/秒）的计算机视觉处理，通常只能降低帧率进行处理，这会限制自动驾驶的移动速度提升。

如果我们选择高性能的处理器，虽然可以带来更高的帧率，但成本和功耗也将成为主要问题。由于自动驾驶车辆和智能机器人是工作在有限机载能源下的移动系统，因此低功耗是非常重要的因素。

目前，有许多软件层面上的技术可以间接优化性能。例如，最简单的方法是降低图像分辨率，但这也会降低图像质量，对自动驾驶车辆的感知和定位能力产生负面影响。因此，我们需要寻找更加高效的算法来处理数据，以在不牺牲精度的情况下降低计算成本和功耗。

而另一种软件层面上的优化技术就是利用并行计算，例如同时使用多个处理器、图形处理单元(GPU)或数字信号处理器(DSP)[2]。这种技术涉及到对一些软件代码片段更加深入细致的重写，这大大加深了开发的难度。但幸运的是，目前已经有许多完善的库来帮助开发者完成这样的工作，例如OpenCL[3]、ARM Compute Library[4]以及基于CUDA并行加速的OpenCV[5]等。

如果说上述的软件层面上的优化技术仍然不能满足读者的计算需求，那读者可以尝试设计自定义的硬件，如参考链接[6,7]中表现的那样。一般来说，为了实现最佳的硬件加速，工程师们会先需要确定计算任务组合 (computing pipeline)的计算瓶颈，并开发专门的硬件来加速关键的计算路径，当然随之带来的是高昂的成本。本书第11章将会对自动驾驶中计算核心的硬件加速方法进行全面回顾。

**6.3 相机标定**

传感器的标定一般指的是确定传感器内参（针孔模型中的相机焦距）和外参（相对于世界坐标系或另一个传感器坐标系的位置以及方向）的参数校准过程。可以这么说，标定是自动驾驶车辆和智能机器人能够在真实场景中完成感知和定位应用的一个重要前提条件。

为了方便读者理解，这里举个例子，在自动驾驶车辆和智能机器人中，为了融合不同传感器的测量结果，如实现相机和IMU的同步定位和映射(SLAM)计算。所有的传感器的测量结果都必须映射到一个共同的坐标系框架下，这就导致我们需要知道传感器与基础坐标系的相对位置。在本节中，作者除了会介绍传感器标定的基本原理，还会介绍一个开源的标定工具--Kalibr[8, 9]。如果说有读者需要更详细的去深入了解传感器校准的技术细节的，可以参考书籍[10]。

**6.3.1 内参**

内参(Intrinsic parameters)的定义一般指的是那些不依赖于外部环境干扰，以及不受传感器安装位置影响的参数。例如在针孔模型中，我们会用![图片](Aspose.Words.cc5469fa-e29d-48b0-91c7-260e90827e7f.004.png)和![图片](Aspose.Words.cc5469fa-e29d-48b0-91c7-260e90827e7f.005.png)来表示特征点(如: 标志点(landmark))在图像平面上的二维投影，其中![图片](Aspose.Words.cc5469fa-e29d-48b0-91c7-260e90827e7f.006.png)、![图片](Aspose.Words.cc5469fa-e29d-48b0-91c7-260e90827e7f.007.png)和![图片](Aspose.Words.cc5469fa-e29d-48b0-91c7-260e90827e7f.008.png)表示了相应特征点在世界坐标系中三维空间中的位置，其中坐标原点位于相机的焦点处。用![图片]变量来表示相机的焦距。在针孔模型中，摄像机的焦距![图片]一般来说是一个内参，通常是未知的或者是只知道大概数值的。所以在使用任何传感器融合算法之前，都需要对该相机模型进行焦距的精确标定。这种操作一般被称为相机的内参标定。与相机类似的还有许多其他传感器，如轮式编码器(wheel encoders)、IMU、LiDAR等，他们都有自身的内参，所以这些传感器在使用前也必须进行内参标定。

**6.3.2 外参**

外参(Extrinsic parameters)的定义一般指的是那些描述传感器相对于参考坐标系的位置和方向(统称为姿态)的参数。如果说，传感器的姿态是以全局坐标系作为参考坐标系的，那估计该姿态参数通常被称为全局定位参数，一般来说，这些不同种类的传感器通常有有效的算法来针对性的标定出外参[10]。

举个例子，如果说我们要对3D相机进行定位，那首先就要对相机姿态的六个自由度进行标定，而这种一般会要至少四个非共线的标志点(landmarks)来完成观测值的计算，而且这些标志点在全局坐标系中的位置应该是已知的，或者需要有至少三条直线是已知的，并且这些直线在三维空间中方向是线性独立的，这样也可以求出相机的六个自由度。

除上面提到的全局标定以外，在许多系统中还会将多个传感器刚性连接到同一个设备上。为了确保系统观测的可靠性以及能够尽可能的鲁邦，降低单个传感器之间转换误差过大的问题，所以需要在多传感器数据融合之前有一个尽可能准确的外参标定的测量值。又因为融合算法只有在传感器空间相关的时候，才能够在同一个坐标系下处理多个传感器输入的测量量。为此我们需要对传感器与传感器的转换进行外参标定，以便使用一个共同的参考坐标系来统一所有传感器测量值的输入位置。这种估计传感器到传感器外参转换的过程一般被我们称为传感器的外参校准。

下面我们来说一下除了相机外参标定以外的其他两个传感器(IMU到GPS)的外参标定情况。通常来说，IMU传感器会被安装在靠近车辆旋转中心的地方以避免饱和(saturation, 模块对一个信号设定上下限)。GPS天线这类的设备则一般会被安装在车辆的外部，以保证可以接收到良好的卫星定位信号。这样的安装位置就会不可避免地导致IMU和GPS之间有很大的距离。假如车辆将IMU传感器作为中心进行原地旋转，在这种情况下，GPS的测量结果会显示线速度不为零，但是IMU根据线加速度的积分得到车辆的速度为零。如果说我们不知道GPS和IMU之间的外参信息，那就会没有办法解决这个矛盾，这就会导致任何融合这两个传感器的算法都会失效。

**6.3.3 Kalibr**

Kalibr是一个在Github上开源的标定工具箱，它可以通过UI界面快速的解决机器人技术中常见的标定问题[8, 9]。首先，Kalibr工具箱可以对多相机进行标定，并可以轻松计算出相机的内参(intrinsic)和外参(extrinsic)，来生成一个非全局共享且具有重叠视场角的相机系统。其次，Kalibr可用于视觉惯性系统的校准(相机-IMU)，一般来说，这种视觉惯性系统通常会被用于自动驾驶车辆和智能机器人的定位任务中。Kalibr作为一个方便的工具，可以快速的生成IMU与相机的校准参数，来将IMU和相机在时间和空间上进行同步。第三点就是Kalibr可被用于多个IMU之间内参的校准。因此，Kalibr是一个非常有用的标定工具，已经被广泛用于自动驾驶车辆和智能机器人的视觉惯性硬件系统的内参和外参的标定。

(1) 标定方法

Kalibr工具箱支持三种不同的标定板。这里作者建议使用Aprilgrid(如图6.4所示)，使用Aprilgrid标定板有以下好处：首先Kalibr支持相机不需要校准板全部可见即可完成该方法的外参标定；其次该标定板使用了YAML文件来完成配置，配置文件必须提供给校准工具。标定板可以在网络上下载，或使用下面的脚本自动生成。值得一提的是，Aprilgrid标定板的图案设计可以有效减少误差和提高标定精度。

kalibr\_create\_target\_pdf –h

(2) 多摄像头标定

第1步：收集图像。通过编写一个机器人操作系统(ROS)包来读取原始图像数据。在整个流程中，多摄像头系统是固定的，而标定板在摄像机前移动，以获得待标定的图像。这里作者建议，在采集待标定数据时，将摄像头的视频流频率降低到4赫兹左右。这可以减少数据集中的冗余信息，从而降低校准所需要的时间。

第2步：运行Kalibr。这里使用下面的指令来运行Kalibr。需要注意的是，由于初始估算的焦距不是准确值，所以在处理完前面几张图片后，优化可能会出现偏差。在这种情况下，请尝试重新启动标定程序，因为程序中处理的图像是随机挑选的。

kalibr\_calibrate\_cameras --bag [filename. bag] --topics[TOPIC\_0 ... TOPIC\_N] --models [MODEL\_0 ... MODEL\_N] --target [target.yaml]

![图片](Aspose.Words.cc5469fa-e29d-48b0-91c7-260e90827e7f.010.jpeg)

图6.4 Aprilgrid标定板

第3步：理解标定结果的输出。这里我们来展示一下Kalibr工具箱标定后产生的结果输出： 

report-cam-%BAGNAME%.pdf：这里指的是PDF格式的报告，当中包含了所有标定结果文档的图片。

results-cam-%BAGNAME%.txt：以文本形式输出标定结果的摘要。 

camchain-%BAGNAME%.yaml：YAML格式的结果，这个文件可以作为camera-IMU标定参数配置的标准输入。

(3) 相机IMU标定

前提：在我们进行相机-IMU标定之前，需要事先对IMU的内参进行校准，并将其校正结果作为相机-IMU标定的原始数据。此外，必须创建一个包含有加速度计和陀螺仪噪声密度(noise density)和偏差随机游走(bias random walk)的YAML文件。

第1步：收集图像。编写一个机器人操作系统(ROS)包来读取原始图像数据以及创建一个包含IMU测量值的CSV文件。其中标定板在这个相机-IMU标定中是固定的，相机-IMU系统会在标定板前方随机移动，这样的操作可以使IMU所有的轴都会存在测量，从而完成IMU所有轴的激活。同时需要确保标定板有着良好的照明，并且保持较低的相机快门时间，以避免过长的曝光时间造成的运动模糊。将摄像机的采样速率设为20赫兹，IMU的采样速率设置为200赫兹，这样就可以获得良好的测量结果。

第2步：运行Kalibr。用下面的命令来运行Kalibr。

kalibr\_calibrate\_imu\_camera --bag [filename. bag] --cam

[camchain.yaml] --imu [imu.yaml] --target [target.yaml]

第3步：理解标定结果的输出。这里我们来展示一下Kalibr工具箱标定后产生的结果输出： 

report-imucam-%BAGNAME%.pdf：这里指的是PDF格式的报告，当中包含了所有标定结果文档的图片。

results-imucam-%BAGNAME%.txt：以文本形式输出标定结果的摘要。 

camchain-imucam-%BAGNAME%.yaml：YAML格式的结果，这个文件是基于上面一小节camchain.yaml文件的参数生成的，这里增加了所有相机相对于IMU的外参变换。

(4) 多IMU内参校准

Kalibr工具箱的扩展版本除了支持本章上面所说的多相机和多IMU的时空校准以外。它还有IMU自身内参估计的功能，同时支持加速度计Y轴和Z轴相对于其X轴的位移的估算。

前提：在我们进行相机-IMU标定之前，需要事先对IMU的内参进行校准，并将其校正结果作为相机-IMU标定的原始数据。此外，必须创建一个包含有加速度计和陀螺仪噪声密度(noise density)和偏差随机游走(bias random walk)的YAML文件。

第1步：收集图像。编写一个机器人操作系统(ROS)包来读取原始图像数据以及创建一个包含IMU测量值的CSV文件。其中标定板在这个相机-IMU标定中是固定的，相机-IMU系统会在标定板前方随机移动，这样的操作可以使IMU所有的轴都会存在测量，从而完成IMU所有轴的激活。同时需要确保标定板有着良好的照明，并且保持较低的相机快门时间，以避免过长的曝光时间造成的运动模糊。将摄像机的采样速率设为20赫兹，IMU的采样速率设置为200赫兹，这样就可以获得良好的测量结果。

第2步：运行Kalibr。用下面的命令来运行Kalibr。

kalibr\_calibrate\_imu\_camera --bag [filename. bag] --cam

[camchain.yaml] --imu [imu.yaml] --target [target.yaml]

除此此外，Kalibr的扩展框架中还有许多附加选项。

--IMU IMU\_YAMLS [IMU\_YAMLS ...]: 这个选项需要读取一个yaml文件列表，因为整个系统中每个IMU都应该有一个yaml配置文件。其中，yaml文件中第一个IMU将会是参考IMU(IMU0)。

--IMU-models IMU\_MODELS [IMU\_MODELS ...]: 这个选项需要获取一个IMU传感器模型的列表，其长度需要与上面的--IMU列表相同。

目前多IMU内参标定支持的模型有已经标定校准的模型(calibrated)、尺度对齐的模型(scale-misalignment)和尺度对齐以及尺寸效应模型(scale-misalignment-size-effect)。在Kalibr中默认选择是已经标定校准的模型(calibrated)。在没有提供模型的情况下，也会假定该模型是已经标定校准的。

第3步：理解输出：在多IMU内参标定种会产生以下输出

report-imucam-%BAGNAME%.pdf：这里指的是PDF格式的报告，当中包含了所有标定结果文档的图片。指的注意的是，在绘制残差的图片中，还会显示三个![图片]界限，这些界限的设置是保存在各自IMU原始数据的yaml文件中的，当然也可以通过选项-reprojection-sigma来设置![图片]界限，从而提供一个假定的过程噪声强度范围，来促使噪声参数和模型有着直观的衡量。 

result-imucam-%BAGNAME%.txt: 这个txt文件包含了所选IMU模型计算出来的具体结果。当中的内容与pdf中的摘要相同。 

imu-%BAGNAME%.yaml：为YAML格式的IMU校准结果。这个文件的内容取决于选项--IMU-models选择的模型。

**6.4 计算机视觉定位**

VSLAM技术被广泛运用于自动驾驶汽车和智能机器人的定位。VSLAM技术能够为自动驾驶汽车和机器人的实际应用提供两个最基本的信息，即“我”在哪里和“我”看到了什么[11]。这类VSLAM视觉定位技术近些年来受到广大开发者和研究者的青睐。虽然这些年来，SLAM的理论体系已经发展的十分成熟了，但是想要将VSLAM技术不仅仅只是停留于理论和仿真研究，而是能较好的应用于实际的自动驾驶汽车或者只能机器人中，这仍需要开发者解决很多实际问题。

**6.4.1 VSLAM概述**

VSLAM系统的设计是以应用为导向的，因为不同的应用场景会对其性能以及精度的要求也不尽相同。例如，对于一个移动机器人来讲，其需要能在大规模环境中进行建图并完成机器人自身的定位，这无疑会给闭环检测以及大规模后端优化带来了极大的挑战。而对于增强现实（AR）和虚拟现实（VR）应用来说，其需要满足高精度、无抖动、低延迟的用户动作位置跟踪的需求，以便用户在观看虚拟内容时，设备可以为其提供沉浸式的体验。对于自动驾驶汽车来说，如何将多个传感器数据融合来实现各种环境下的高精度定位是问题的关键，这无疑给传感器的实时融合跟踪带来了极大的挑战。因此，现在SLAM能否在实际应用中发挥实际作用，这取决于传感器的选择。所以现在主流的传感器选择会依赖目标的应用场景来专门设计一个应用系统，并根据需求设计具体的实施细节。

目前主流的SLAM算法是伴随着传感器技术和计算平台发展的。一开始，SLAM技术主要应用于配备了轮速编码器和测距测量传感器的机器人上。这时的SLAM系统主要是基于卡尔曼滤波器（Kalman filter）[12]的，通过将带有高斯噪声（Gaussian noise）的近似线性模型进行滤波处理来和SLAM系统联合估计出机器人当前姿态和地图（比如一组地标），也有一部分机器人会选择使用粒子滤波算法（particle filter）[13]来构建出多个假设（即粒子）以通过粒子的分布来完成机器人在全局地图中的定位。

一些被动触发式的传感器，例如CCD/CMOS传感器，这类传感器的正在变得物美价廉，也被各类厂商大规模的生产，因此，最近几年许多使用稀疏特征的VSLAM方法（例如PTAM[14],ORB-SLAM[15]）得到了快速发展。尽管绝大多数的SLAM方法是通过图像特征来建立一个稀疏的点云的，但是仍有一些SLAM方法通过三维刚体与相似变换将图像到图像对齐来直接估计出相机自身的姿态和深度信息（如LSD-SLAM[16]），从而构建出一个更加密集的点云地图。

最近几年，使用VIO算法成为了主流，其作为一种将IMU与视觉融合，并用于定位任务的传感器融合方法，研究者发现通过视觉传感器和IMU的结合可以获得更加高效的效果。在这个系统中，IMU可以提供高帧率的姿态估计，而相机（视觉传感器）则可以在前者的基础上提供精确的姿态校正和地图构建。视觉-惯性系统（visual-inertial systems）大致可以分为两类：紧耦合系统[17]和松耦合系统[18]。紧耦合系统联合最优化了惯性imu和视觉传感器的测量结果，因此可以在建图和跟踪任务中取得更高的精度。而相较于前者，松耦合系统则对于传感器之间的融合则显得更加灵活，因此其对于时间戳同步和计算成本的要求较低。

**6.4.2  ORB-SLAM2**

ORB-SLAM2是一个支持单目、双目、RGB-D相机的开源SLAM框架，能够实时计算相机的位姿并同时对周围环境进行稀疏地图的三维重建。该算法可以实时的完成回环检测和相机的重定位[19]。

在ORB-SLAM2开源项目中[20]，作者提供了一个以KITTI数据集为例的ORB-SLAM2系统。在该数据集中支持ORB-SLAM2单目相机或双目立体相机两种模式运行。而在TUM数据集中，ORB-SLAM2则可以使用RGB-D相机或单目相机这两种模式运行该数据集。在EuRoC数据集中，ORB-SLAM2可以选择双目立体相机或单目相机模式完成SLAM定位和建图。为了方便管理数据流的通信，作者还提供了一个能够实时处理单目相机、双目立体相机和RGB-D相机流的ROS节点，当然ORB-SLAM2算法框架是不依赖ROS的，所以读者可以根据自身的需求对ORB-SLAM2算法选择合适的编译方式。除此以外，ORB-SLAM2算法还提供了一个图形化的用户界面，使用者可以在SLAM模式和定位模式之间进行切换。

**步骤1 准备工作**

在安装ORB-SLAM2之前，我们需要先在电脑上做一些准备工作，来将所需的程序依赖库完成安装。第一点需要注意的是，因为作者将ORB-SLAM2在Ubuntu 14.04和16.04的系统上已经成功运行，所以这里建议在上述两个Ubuntu系统中选择一个版本进行安装。第二点需要注意的是ORB-SLAM2使用了C++11中的多线程库和时间库，需要使用C++11或C++0x编译器对其进行编译，因此我们需要选择上述两个编译器并编译其中之一。第三点需要注意的是ORB-SLAM2使用Pangolin来实现可视化和用户界面，因此我们需要确保安装了Pangolin库。第四点需要注意的则是ORB-SLAM2使用OpenCV进行图像和特征处理，因此我们需要确保安装了OpenCV库。第五点需要注意的是ORB-SLAM2使用Eigen3进行矩阵计算（主要用于g2o进行非线性优化），因此我们还需要确保安装了Eigen3库。最后，因为ORB-SLAM2使用的DBoW2库和g2o库都已经存放在了ORB-SLAM2的第三方文件夹中，所以我们无需另行安装。其中DBoW2库是用于回环检测进行位置识别，而g2o库是用于进行非线性优化。

**步骤2 构建ORB-SLAM2库**

下面，我们将在本地构建一个ORB-SLAM2库。首先通过如下命令来克隆ORB-SLAM2仓库

git clone https://github.com/raulmur/ORB\_SLAM2.git ORB\_SLAM2

在ORB-SLAM2中还提供了一个方便的脚本build.sh来快速构建第三方库和ORB-SLAM2。在进行构建工作之前，须要确保你的系统中已经安装了步骤1 中所述的所有必要依赖，然后执行如下命令以构建ORB-SLAM2库

cd ORB\_SLAM2

chmod +x build.sh

./build.sh

这些命令会在lib文件夹下创建一个名为libORB\_SLAM2的.so文件，并在Examples文件夹下创建一些可执行文件，如mono\_ tum、mono\_kitti、rgbd\_tum、stereo\_kitti、mono\_euroc和stereo\_euroc等。

**步骤3 运行双目立体摄像头数据集**

KITTI作为一个非常著名的自动驾驶算法的数据集[21]。该数据集包含一个长达了六小时的真实车辆采集的交通场景，其采样频率从10-100hz不等，并且该数据集中使用了各种传感器进行了数据采集，例如高分辨率彩色相机和灰度双目立体相机、Velodyne三维激光扫描仪和高精度GPS/IMU惯性导航装置等。因为其获取的是真实世界中的各种交通场景，所以KITTI数据集能够给使用者提供各种各样的使用场景，你可以根据需求去选择使用人员稀少农村地区的高速公路或者是有许多静态和动态物体的城市内部道路。并且KITTI中提供的传感器数据是经过了校准、时间同步和标记的，并额外提供了校准后和原始的两种图像序列供使用者选择。

在本小节中，我们将会向你展示如何在KITTI中的双目立体数据集中去运行ORB-SLAM2。该数据集可以从下面的链接进行下载使用。

<http://www.cvlibs.net/datasets/kitti/eval_odometry.php>

我们则可以执行以下命令来在数据集中运行ORB-SLAM2，为了使得文件名称更加便于理解，我们可以将KITTIX.yaml改名为KITTI00-02.yaml、KITTI03.yaml或KITTI04-12.yaml来分别代表序列号为0-2、3和4-12的数据集。然后将其中的PATH\_TO\_DATASET\_FOLDER改为KITTI数据集所在文件夹的路径，并将SEQUENCE\_NUMBER改为00、01、02、...、11等(即在sequence/后面加上需要的序号）。

./Examples/Stereo/stereo\_kitti Vocabulary/ORBvoc.txt Examples/Stereo/KITTIX.yaml PATH\_TO\_DATASET\_FOLDER/dataset/sequence

**6.5 计算机视觉感知**

在本节中，我们将介绍两种使用计算机视觉感知的技术：双目图像估算深度以及目标实例分割。通过目标实例分割技术，我们可以从图像中提取出检测到的物体语义信息，例如检测到的目标到底是汽车还是行人。在该技术基础之上，我们还可以通过深度估计技术来计算出被检测到的目标的深度信息（即离相机的距离）。通过将这两个技术进行结合，系统可以做到正确识别环境中所有目标物体的空间信息（距离）和语义信息（种类），该信息是规划和控制模块做出最终决策中必不可少的一环。

为了让VSLAM能够更好地在自动驾驶汽车和智能机器人领域中发挥作用，我们对双目深度估计算法的运行速率提出了很高的要求。只有算法的运行帧数每秒钟大于30帧时，才能保证自动驾驶汽车在正常行驶是能够使用该算法。另一方面，估计出的深度的误差会随着距离的增加而快速增大，所以我们通常需要选用高分辨率的图像，来保证摄像头获取到物体的三维位置信息的精度[20]，但随之带来的是，高分辨率的图像会给系统的计算能力提出更高的要求。因此，这一问题的关键在于如何能在不牺牲深度精度的情况下还能满足汽车的正常驾驶的需求。

基于局部特征的双目立体视觉匹配算法的实时性通常很好，但是我们需要选择合适的局部窗口[23]。如果窗口选的太小会导致窗口之间的匹配率较低，而窗口选的较大则会导致边界出现“伪影”（指物体两边的边界逐渐消失），所以我们需要在两者之间做出权衡。因此，该方法很难处理纹理不清晰或者表面模糊的物体。另一方面，我们可以通过全局匹配算法来获得密集且精准的匹配效果。该算法通过最小化基于MRF的能量函数来显示地提升其匹配的平滑度[24]。但是全局匹配算法需要极高的计算能力。在本章节的后半部分，我们将介绍ELAS算法（Efficient LArge-scale Stereo），该算法在保证输出密集且精确的匹配效果的同时，还能利用双目相机快速地估算出深度[25]。

在目标检测领域，R-CNN作为一种使用边框的目标检测方法，其主要用于寻找一定数量范围内的候选目标区域，并在每个感兴趣区域（ROI）中分别使用卷积神经网络模型识别出候选区域的类型[26]。相较于R-CNN，Faster R-CNN引入了一个区域候选网络（Region Proposal Network，RPN），该网络与检测网络共享输入图像的卷积特征[27]。在这些算法的流程中，网络是先进行图像的分割然后再进行图像的识别的，这就导致了这些网络识别速度慢且准确率低。在6.5.2小节中，我们将会介绍Mask R-CNN算法，该算法是基于mask掩模和类标签的并行预测，相较于上述提到的方法，Mask R-CNN更加简单且更加灵活[28]。

**6.5.1 双目立体深度感知算法—ELAS**

正如《Efficient large-scale stereo matching》[25]一文中介绍的，ELAS是一个用于双目立体视觉匹配的生成式概率模型，通过减少匹配关系上的歧义性来允许使用小聚合窗口进行密集匹配。ELAS通过在一组鲁棒匹配对应项上形成一个三角形（将其命名为“支持点”）以在视差空间上建立先验。由于建立的先验是分段线性的，所以该算法不会在纹理较差和存在倾斜的表面的情况中出现问题。因此，ELAS是一种高效的算法，其减少了搜索空间，并可以很容易实现并行操作。ELAS的作者证明了ELAS能够实现最先进的性能，与主流的方法相比，ELAS速度显著的提高了三个数量级。

LIBELAS (LIBrary for Efficient LArge-scale Stereo matching)是一个跨平台的C++库，该库主要用于计算出大型图像的视差图（disparity maps）。例如，你可以向LIBELAS输入一对大小相同的矫正后的灰度双目立体图像来让生成其对应的视差图。

在编译LIBELAS之前，请确保你系统内已经安装好了CMake（如果没有安装，可在该网址进行下载和安装：<http://www.cmake.org>），在使用C++去编译LIBELAS时CMake是必须的。如果你使用的是Liunx系统，可以通过以下步骤来编译LIBELAS：

步骤一：移动到LIBELAS根目录

步骤二：键入'cmake .'

步骤三：键入'make'

步骤四：运行'./elas demo'

通过上述步骤，系统可以计算出“image”路径下图像的视差图。

**6.5.2 目标实例分割算法—Mask R-CNN**

实际上，Mask R-CNN算法是Faster R-CNN算法的一个延伸，相较于后者，其增加了一个用于预测每个ROI的分割掩模的分支。该分支与现有的分类和边界框回归的分支并行运行[28]。相较于Faster R-CNN算法，Mask R-CNN算法的训练和实现更加容易，这归因于其灵活的架构设计。此外掩模分支仅增加了少量计算开销，但是提升了整个系统的效率，这使得搭建一个系统速率满足自动驾驶的需求成为可能。

从原理上来看，Mask R-CNN算法是Faster R-CNN算法的一个直观扩展，前者通过正确地构建建掩模分支来取得了一个更好地效果。更重要的是，Faster R-CNN算法并不是为了满足网络输入和输出之间像素级的一一对齐而设计的。而Mask R-CNN算法增加了一个简单的、无量化的层（通常被称为ROI Align），该层可以精确的保留图像原来真实的空间位置。ROI Align使得掩码精度从10%提升到了50%。除此以外，Mask R-CNN算法还可以实现掩码和类别预测的解耦，其会独立地为类别（ROI）输出一个二进制掩码，这些类别之间不存在竞争，仅依靠网络的ROI分类分支来预测。

开源版本Mask R-CNN算法可以在github上的Mask R-CNN项目中找到[30]（该项目的链接：<https://github.com/matterport/Mask_RCNN>），该项目中包含不同框架下的Mask R-CNN算法包（例如Python 3、Keras和TensorFlow等）。该模型可以为图像中的物体实例生成边框和分割掩模mask。该模型以特征金字塔网络（FPN）和ResNet101为主干网络（backbone）。该代码库包含的内容如下所示：一个基于FPN和ResNet101的Mask R-CNN算法的源代码、 基于MS COCO数据集（Microsoft Common Objects in Context，是微软团队获取的一个可以用来图像recognition+segmentation+captioning 数据集）的训练代码、基于MS COCO数据集的预训练权重、为多GPU训练提供的并行模型分类、 基于MS COCO数据集的评估（应用处理器 AP）。

我们可以通过以下步骤来安装Mask R-CNN代码库

输入命令如下所示

git clone https://github.com/matterport/Mask\_RCNN 

pip3 install -r requirements.txt

python3 setup.py install

此外我们还可以从	<https://github.com/matterport/Mask_RCNN/releases>中下载基于MS COCO数据集的预训练权重 (mask\_rcnn\_coco.h5)

如果想要在MS COCO数据集中进行代码的训练和测试，则需要在<https://github.com/waleedka/coco>中下载并安装pycocotools。

**6.6 DragonFly系统的计算机视觉模块**

DragonFly系统的传感器模块如图6.5所示，PerceptIn公司的DragonFly系统利用将计算机视觉与其它传感器进行融合来获得汽车或机器人精确的位置和并对周围进行感知，以满足户外低速自动驾驶汽车的需求[31,32]。下面我们来简单介绍一下该计算机视觉模块的具体组成部分，其包含的设备如下所示：四个硬件同步的高清全局快门相机（前后各一对双目立体相机）、一个IMU设备、一个Jetson TX1计算模块和一个GNSS接收模块（Global Navigation Satellite System，全球卫星导航系统，并通过GNSS接收模块拿到自动驾驶汽车的三维坐标和速度以及时间信息）的接口。

![图片](Aspose.Words.cc5469fa-e29d-48b0-91c7-260e90827e7f.012.jpeg)

图6.5 DragonFly系统的传感器模块（IMU for Visual Inertial Odometry：用于VIO的IMU 4HD Global Shutter Camera：4个高清全局快门相机 GNSS Module：GNSS模块 Powered by NVIDIA JETSON: 由NVIDIA JETSON进行供电 ）

相较于其他计算机视觉模块，该模块具有以下优势。DragonFly系统的计算机视觉模块不仅能够实现相机间的硬件同步，而且还可以实现IMU的硬件同步。该模块将很多SLAM数据进行预处理，从而降低了开发者通过该设备进行视觉SLAM开发的难度。除此以外，该模块的双目立体相机对具有50厘米的基线(两个相机的光心之间的距离)，其远远大于一般计算机视觉模块中相机的基线的长度。这也使得该模块可以实现远距离的感知，该模块甚至可以探测300m以外的物体[33]。然而，该模块也存在着缺陷。多传感器和长距离基线相机的设计大大增加了该模块的校准难度。因此我们单独出了一段视频用于介绍该模块在进行校准时的一些细节（校准视频链接：<https://www.youtube.com/watch?v=tRhGStjnS6M>）[34]。

**6.6.1 DragonFly定位接口**

在该系统中，DragonFly传感器模块通过运行PerceptIn公司开发的特殊的VIO算法来获得精确的汽车位置和方位信息。 然而，VIO算法本身存在着累积误差的缺陷。汽车行驶的距离越长，累积误差越大，其位置信息就越不准确。因此，仅靠VIO算法是没办法实现长期获取汽车位置的准确信息。

为了解决上述累积误差的问题，我们可以将GNSS的实时动态定位（real-time kinematic，RTK）结果与VIO定位结果相结合。当GNSS信号较好时，RTK GNSS可以为自动驾驶汽车提供非常精确的位置信息，该信息可以用于纠正和减少VIO算法累积的位置误差。另一方面，当GNSS信号较差或者发生多路径效应（multipath problems，会使得汽车接收机接收到的卫星信号为失真的卫星发射信号）时，此时通过VIO算法来为自动驾驶汽车提供精确的位置信息。在过去的十几年中，我们通过大量实验验证了该方案的有效性。

下面的数据结构能够显示我们是如何将VIO定位结果和GNSS定位结果相融合到一起的。 VIO算法不断计算出相对于起始位置的局部坐标系位置（tx, ty, tz）和四元数（qx, qy, qz, qw）。为了能提高VIO和GNSS之间的兼容性，VIO每次更新完这些位置数据之后，我们都需要将其转换为通用墨卡托（Universal Transverse Mercator，UTM）格式（utm\_x, utm\_y, heading），该数据格式是GNSS设备所使用的绝对坐标。

当GNSS信号良好时，GNSS的定位信息被直接反馈到控制和规划模块用于汽车的实时定位，与此同时，该信息也会被反馈到定位模块中的扩展卡尔曼滤波器（extended Kalman filter）以纠正和减少VIO的累积误差。而当GNSS信息较差时，系统会先将VIO定位信息转换为UTM格式，然后再将其反馈给控制和规划模块用于完成自动驾驶汽车的控制。

typedef struct PILocalizationVioMsg\_

{

// Sensor module's timestamp, in the unit of second 

double stamp;

double gstamp; // gps global timestamp

// position of DragonFly under the world coordinate 

double tx;

double ty;

double tz;

// orientation (quaternion) of DragonFly under the world coordinate;

double qy;

double qz;

double qw;

// UTM coordinates, valid when gnss\_fusion is enabled 

double utm\_x;

double utm\_y;

int utm\_zone;

double gheading;      // heading from GPS

// gnss fusion pos\_status

int fusion\_mode;     // 0: gps bypass; 1: vio fusion

int pos\_status;      // position status under gps bypass mode

int heading\_status;  // heading status under gps bypass mode

double accuracy;     // 0.0 under gps bypass mode; represents the accuracy

`                     `// under vio fusion mode.

} PILocalizationVioMsg;

**6.6.2 DragonFly感知接口**

在该系统中，DragonFly传感器模块通过运行PerceptIn公司开发的感知算法来获得障碍物精准的空间信息和语义信息。空间信息指的是我们通过双目立体视觉（这里指代的是自动驾驶汽车上的双目立体相机对）来检测目标物体相对于汽车车头中心的距离。而语义信息指的是我们使用深度学习等方法提取出的目标物体的类型（例如行人、自行车、汽车等）。感知系统可以将两者信息结合起来以获取不同类型的物体与自动驾驶汽车之间的距离。最终，规划和控制模块依据这些信息和汽车当前状态信息（比如车速）来做出智能决策以保证自动驾驶汽车在行驶过程中的安全。

除此以外，感知系统还结合了主动感知数据（来源于DragonFly传感器模块）与被动感知数据（来源于雷达和声呐），从而为汽车提供当前环境的全方位的感知。

感知模块应用程序接口（API）及其相关参数结构如下所示：

Perception API:



unsigned int GetObstacles(Perception\_Obstacles \*perceptionObs);



Return value:

The return value is a 32bit unsigned int. The meaning of each type in the return data is defined as follows,

reserved    |  sonar        |  radar      |  vision bit[31-24]          | bit[23-16]   |   bit[15-8]   |  bit[7-0]



Data structure:

typedef struct \_PerceptionObstacle {

SensorType sensor\_type;         // SensorType: Enum Type to represent

// Radar, Sonar, Vision

int sensor\_id;                   // There are multiple Radars/Sonars

int obj\_id;                      // Obstacle id double timestamp;

Pose3D pose;                   // Obstacle position in vehicle

// coordinate

Arc2D arc;                       // Sonar output arc

Velocity3D velocity;              // Obstacle velocity

float power;                     // Reflection power of Radar ObstacleType obs\_type;                          // ObstacleType: Enum type to represent

// the class of an obstacle

double confidence;               // Confidence level of the detection type

// and result in terms of percentage std::vector<Point2D> obs\_hull;     // 2D Vision detected obstacle

// ostream operator overload enable std::cout friend std::ostream &operator<<(std::ostream &os,

const \_Perception\_Obstacle &pObs);

} Perception\_Obstacle, \*PPerception\_Obstacle;

Perception\_Obstacle中的一些具体定义如下所示：

typedef struct \_Pose3D {

double x;        // unit: meter

double y;        // unit: meter

double z;        // unit: meter

double heading;  // in vehicle coordinate, unit: radian

} Pose3D, \*PPose3D;





typedef struct \_Arc2D {

// (x, y) is center of the arc. double x; // unit: meter

double y;           // unit: meter double start\_angle;         // unit: radian double end\_angle;          // unit: radian double radius;             // unit: meter

} Arc2D, \*PArc2D;





typedef struct \_Velocity3D { double vel\_x;              // unit: meter/s double vel\_y;         // unit: meter/s double vel\_z;         // unit: meter/s

} Velocity3D, \*PVelocity3D;





enum ObstacleType : char { UNKNOWN = 0,

UNKNOWN\_MOVABLE = 1,

UNKNOWN\_UNMOVABLE = 2,

PEDESTRIAN = 3,  // Pedestrian BICYCLE = 4,                // bike, motor bike.

VEHICLE = 5,     // Passenger car, bus or truck.

};

**6.6.3 DragonFly+系统**

为了提升自动驾驶汽车的安全性和可靠性，我们对下一代DragonFly提出如下四个基本要求：

模块化：需要配备一个能够通过计算机视觉进行定位和测绘的独立的硬件模块

SLAM优化：需要更好地将四台相机与IMU进行硬件同步

低功耗：将系统的总功耗限制在10W以下

高性能：在720P的YUV图像中运行帧数需要大于30

需要注意的是，在此结构设计下，即使其处理速度只有30帧/秒，系统每秒也会产生100MB左右的原始图像数据，因此该方法对系统的计算能力提出了极高的要求。通过初步的分析计算，我们发现图像的前端处理（比如图像的特征提取）所花费的时间了总处理时间的80%以上。

为了满足上述设计要求，我们设计并实现了DragonFly+，这是一个基于FPGA的实时定位模块[35]。其具体结构如图6.6所示。DragonFly+系统具有如下特点：（1）实现了四个图像通道与IMU（Inertial Measurement Unit）之间的硬件同步（2）通过无缓冲IO架构（direct IO architecture）来减少片外存储通讯（3）通过全流水架构（fully pipelined architecture）来加块定位系统在图像前端处理时的速度。并且该系统还采取了并行和复用处理技术，该技术能够有效缓解带宽和硬件对资源的消耗。

![图片](Aspose.Words.cc5469fa-e29d-48b0-91c7-260e90827e7f.013.jpeg)

图6.6 DragonFly+系统的硬件结构（Camera Kit：相机套件 Camera 1：相机1 Camera 2：相机2 Camera 3：相机3 Camera 4：相机4 MIPI To Parallel Port Converte：MIPI并行转换端口 Trigger signal：触发信号 Images @30fps per channel：图像通道速率30FPS/s FPGA Board：FPGA板 Timer：计时器 Parallel Port and Image Sync. Logic：并行端口和图像同步处理单元 Serial Interf：串行接口 IMU：惯性测量单元 IMU data @240fps：IMU数据帧240FPS/s Sync.Images：同步图像 vSLAM Front End Logic：vSLAM进行前端处理 Features and Depth：物体的特征和深度信息 Mem. Controller：内存控制器 DRAM：动态随机存取内存 Multi-core CPUs：多核CPUs DMA：直接存储器访问 ）

我们对DragonFly+硬件系统进行性能评估，并将其与Nvidia TX1 GPU片上系统（SoC）和英特尔酷睿i7处理器上进行比较。结果如下，在系统处理四路高清图像（图像分辨率为720p）时，DragonFly+的运行速度为42 FPS/s，并且其消耗功率近为2.3W，该性能完全满足我们的设计目标。与其相比。Nvidia Jetson TX1 GPU SoC运行速度为9 FPS/s，其功耗为7W，而英特尔酷睿i7运行速度为15 FPS/s，其功耗为80W。因此相较于Nvidia TX1，DragonFly+能耗效率提高3倍，计算能力提升5倍，而与英特尔酷睿i7相比较，DragonFly+能耗效率提高34倍，计算能力提升3倍。

**参考文献**



1. ` `YouTube (2017). Enabling Computer-Vision-Based Autonomous Vehicles. ht[tps://www](http://www/). youtube.com/watch?v=89giovpaTUE&t=62s (accessed 1 October 2019).
1. Tang, J., Sun, D., Liu, S., and Gaudiot, J.L. (2017). Enabling deep learning on IoT devices.*Computer* 50 (10): 92–96.
1. Khronos (2019). OpenCL Overview. ht[tps://www](http://www.khronos.org/opencl).khr[onos.org/opencl](http://www.khronos.org/opencl) (accessed 1 October  2019).  
1. Arm (2017). ARM Compute Library. ht[tps://www.arm.com/wh](http://www.arm.com/why-arm/technologies/)y[-arm/technologies/](http://www.arm.com/why-arm/technologies/) compute-library (accessed 1 October 2019).
1. OpenCV (2019). CUDA. <https://opencv.org/cuda> (accessed 1 October 2019).
1. Fang, W., Zhang, Y., Yu, B., and Liu, S. (2017). FPGA-based ORB feature extraction for real-time visual SLAM. In: *2017 International Conference on Field Programmable Technology (ICFPT)*, 275–278. IEEE.
1. Tang, J., Yu, B., Liu, S. et al. (2018). π-SoC: heterogeneous SoC architecture for visual inertial SLAM applications. In: *2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*, 8302–8307. IEEE.
1. Rehder, J., Nikolic, J., Schneider, T. et al. (2016). Extending kalibr: calibrating the extrinsics of multiple IMUs and of individual axes. In: *2016 IEEE International Conference on Robotics and Automation (ICRA)*, 4304–4311. IEEE.
1. GitHub (2017). Kalibr. <https://github.com/ethz-asl/kalibr> (accessed 1 May 2019).
1. Mirzaei, F.M. (2013). Extrinsic and intrinsic sensor calibration. PhD thesis. University of Minnesota.
1. Cadena, C., Carlone, L., Carrillo, H. et al. (2016). Past, present, and future of simultaneous localization and mapping: toward the robust-perception age. *IEEE Transactions on Robotics* 32 (6): 1309–1332.
1. Kalman, R.E. (1960). A new approach to linear filtering and prediction   problems. *Journal of Basic Engineering* 82 (1): 35–45.
1. Thrun, S. (2002). Particle filters in robotics. In: *Proceedings of the Eighteenth Conference on Uncertainty in Artificial Intelligence*, 511–518. Morgan Kaufmann Publishers Inc.
1. ` `Klein, G. and Murray, D. (2007). Parallel tracking and mapping for small AR workspaces.In: *Proceedings of the 2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality*, 1–10. IEEE Computer Society.
1. Mur-Artal, R., Montiel, J.M.M., and Tardos, J.D. (2015). ORB-SLAM: a versatile and accurate monocular SLAM system. *IEEE Transactions on Robotics* 31 (5): 1147–1163.
1. Engel, J., Schöps, T., and Cremers, D. (2014). LSD-SLAM: large-scale direct monocular SLAM. In: *European Conference on Computer Vision*, 834–849. Cham: Springer.
1. Qin, T., Li, P., and Shen, S. (2018). Vins-mono: a robust and versatile monocular visual- inertial state estimator. *IEEE Transactions on Robotics* 34 (4): 1004–1020.
1. Lynen, S., Achtelik, M.W., Weiss, S. et al. (2013). A robust and modular multi-sensor fusion approach applied to MAV navigation. In: *2013 IEEE/RSJ International Conference on Intelligent Robots and Systems*, 3923–3929. IEEE.
1. Mur-Artal, R. and Tardós, J.D. (2017). ORB-SLAM2: an open-source SLAM system for monocular, stereo, and RGB-D cameras. *IEEE Transactions on Robotics* 33 (5): 1255–1262.
1. GitHub (2017). ORB-SLAM2. <https://github.com/raulmur/ORB_SLAM2> (accessed 1 May 2019).
1. Geiger, A., Lenz, P., Stiller, C., and Urtasun, R. (2013). Vision meets robotics: the KITTI dataset. *The International Journal of Robotics Research* 32 (11): 1231–1237.
1. Gallup, D., Frahm, J.M., Mordohai, P., and Pollefeys, M. (2008). Variable baseline/ resolution stereo. In: *2008 IEEE Conference on Computer Vision and Pattern Recognition*, 1–8. IEEE.
1. Scharstein, D. and Szeliski, R. (2002). A taxonomy and evaluation of dense two-frame stereo correspondence algorithms. *International Journal of Computer Vision* 47 (1–3): 7–42. 
1. Felzenszwalb, P.F. and Huttenlocher, D.P. (2006). Efficient belief propagation for early vision. *International Journal of Computer Vision* 70 (1): 41–54.
1. Geiger, A., Roser, M., and Urtasun, R. (2010). Efficient large-scale stereo matching. In:*Asian Conference on Computer Vision*, 25–38. Berlin: Springer.
1. Girshick, R., Donahue, J., Darrell, T., and Malik, J. (2014). Rich feature hierarchies for accurate object detection and semantic segmentation. In: *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, 580–587. IEEE.
1. Ren, S., He, K., Girshick, R., and Sun, J. (2015). Faster R-CNN: towards real-time object detection with region proposal networks. In: *Advances in Neural Information Processing Systems*, 91–99. Neural Information Processing Systems Foundation, Inc.
1. He, K., Gkioxari, G., Dollár, P., and Girshick, R. (2017). Mask R-CNN. In: *Proceedings of the IEEE International Conference on Computer Vision*, 2961–2969. IEEE.
1. Geiger, A. (2019). LibELAS. <http://www.cvlibs.net/software/libelas> (accessed 1 May 2019).
1. GitHub (2017). Mask R-CNN. <https://github.com/matterport/Mask_RCNN> (accessed 1 May 2019).
1. YouTube (2018). DragonFly Sensor Module. ht[tps://www](http://www.youtube.com/watch?v=WQUGB-).y[outube.com/w](http://www.youtube.com/watch?v=WQUGB-)at[ch?v=WQUGB](http://www.youtube.com/watch?v=WQUGB-)- IqbgQ (accessed 1 October 2019).
1. PerceptIn (2018). PerceptIn’s DragonFly Sensor Module. ht[tps://www](http://www.perceptin.io/).per[ceptin.io/](http://www.perceptin.io/) products (accessed 1 May 2019).
1. YouTube (2019). 300-meter Visual Perception Systems for Autonomous Driving. https:// [www.youtube.com/watch?v=2_VfLZLy7Eo&t=21s](http://www.youtube.com/watch?v=2_VfLZLy7Eo&amp;t=21s) (accessed 1 October  2019).
1. YouTube (2018). (DragonFly Calibration. ht[tps://www](http://www.youtube.com/watch?v=tRhGStjnS6M).y[outube.com/w](http://www.youtube.com/watch?v=tRhGStjnS6M)at[ch?v=tRhGStjnS6M](http://www.youtube.com/watch?v=tRhGStjnS6M) &feature=youtu.be (accessed 1 October 2019).
1. Fang, W., Zhang, Y., Yu, B., and Liu, S. (2018). DragonFly+: FPGA-based quad-camera visual SLAM system for autonomous vehicles. *HotChips 2018*, Cupertino, CA (19–21 August 2018). IEEE.


[图片]: Aspose.Words.cc5469fa-e29d-48b0-91c7-260e90827e7f.009.png
[图片]: Aspose.Words.cc5469fa-e29d-48b0-91c7-260e90827e7f.011.png
